{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Love Song Challenge",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izpqp0Mb-d_m",
        "colab_type": "text"
      },
      "source": [
        "# Love Song Lyrics Generator Challenge\n",
        "One of my peers challenged us to create a lovesong based on a dataset that was provided. The dataset contained lyrics from various lovesongs. The implementation that I have chosen is by using a Character-Level LSTM that would generate the lyrics character-by-character.\n",
        "\n",
        "Let's first load our dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuXLxVko_mQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idkvnP1mIpu-",
        "colab_type": "text"
      },
      "source": [
        "The next step is to retrieve the dataset provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EELLZOoeIxmm",
        "colab_type": "code",
        "outputId": "f42f9c73-c08d-417a-a796-90d834351256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('drive/My Drive/')\n",
        "\n",
        "with open('Output1.txt', 'r') as f:\n",
        "    original_data = f.read()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZCAAUS5I0aZ",
        "colab_type": "text"
      },
      "source": [
        "To add a little twist to the dataset. I've decided to load in a dataset provided by *Sergey Kuznetsov* from Kaggle which is the 55000+ Song Lyrics dataset. Link to the dataset: https://www.kaggle.com/mousehead/songlyrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ubiTO_7KTj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('SongLyrics/')\n",
        "data = pd.read_csv('songdata.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8tOYztiKZe5",
        "colab_type": "text"
      },
      "source": [
        "Since most of these songs are not related to lovesongs at all. We may end up with a model that produces lyrics from multiple genres. We want to make sure that the songs that we use are only love-songs. Let's retrieve only the songs with the word \"love\" in it. Let's also pre-process it and remove the commas and periods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vCDQBHNKx2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = data[data['song'].str.lower().str.contains('love')].copy()\n",
        "dataset['text'] = dataset['text'].apply(lambda x: x.replace(',', ''))\n",
        "dataset['text'] = dataset['text'].apply(lambda x: x.replace('.', ''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcvShXisK5hj",
        "colab_type": "text"
      },
      "source": [
        "Now let's combine the original dataset and the Kaggle dataset into one single list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPThFnl4K_4D",
        "colab_type": "code",
        "outputId": "5ffda7ab-2b74-4624-c476-a8add85ae46b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "text = ' '.join(dataset['text'])\n",
        "text = original_data + ' ' + text\n",
        "print(\"length of original dataset \" + str(len(original_data)))\n",
        "print(\"length of original+kaggle dataset \" + str(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of original dataset 902550\n",
            "length of original+kaggle dataset 4746168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyKjZRq5LR7h",
        "colab_type": "text"
      },
      "source": [
        "We've increased the original dataset's length by more than 5 times! Now let's add in the pre-processing step by tokenizing our dataset and creating our mapping dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLUtcxB9M1vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded = [char2int[m] for m in text]\n",
        "encoded = np.array(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx7BcxcjNCcm",
        "colab_type": "text"
      },
      "source": [
        "Let's add a get_batches() function that would allow us to split our initial sequence into batches and iterate through it through a window called the sequence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqO9P6nyNfbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    \n",
        "    n_batches = int(len(arr) / (seq_length * batch_size))\n",
        "  \n",
        "    arr = arr[:int(batch_size * n_batches * seq_length)]\n",
        "    \n",
        "    arr = arr.reshape(batch_size, -1)\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:,n:n+seq_length]\n",
        "        y = arr[:,n+1:(n+1)+seq_length]\n",
        "        if len(y[0]) != len(x[0]):\n",
        "            z = np.zeros((batch_size,1), dtype=np.int64)\n",
        "            y = np.append(y, z, axis=1)\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R90Q_q3NjpS",
        "colab_type": "text"
      },
      "source": [
        "Now let's check if we can train on GPU then define our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xDziL_kNmtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        self.lstm = nn.LSTM(len(tokens), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "  \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                \n",
        "        r_out, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        r_out = self.dropout(r_out)\n",
        "        r_out = r_out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(r_out)\n",
        "  \n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcnGivaVNrZm",
        "colab_type": "text"
      },
      "source": [
        "Let's add in our training loop that would allow us to optimize our network's parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phcpfWkNUGF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    step_plot = [] # We'll use these for plotting later\n",
        "    train_loss_plot = []\n",
        "    valid_loss_plot = []\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    valid_loss_min = np.Inf # We want to save our best model with the lowest validation loss\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            x = torch.from_numpy(x)\n",
        "            # Use functional library's one_hot function to one-hot encode our data.\n",
        "            x = F.one_hot(x, num_classes=n_chars)\n",
        "\n",
        "\n",
        "            targets = torch.from_numpy(y)\n",
        "            inputs = x.float()\n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "            net.zero_grad()\n",
        "\n",
        "            output, h = net(inputs, h)\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # Check if step count has reached to calculate for validation loss\n",
        "            if counter % print_every == 0:\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                # Set to evaluation mode so there won't be any gradient calculation.\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    x = torch.from_numpy(x)\n",
        "                    x = F.one_hot(x, num_classes=n_chars)\n",
        "                    y = torch.from_numpy(y)\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    inputs, targets = x.float(), y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train() # Go back to training\n",
        "\n",
        "                print(\"Epoch: \" + str(e+1) + \" Train Loss: \" + str(loss.item()) \\\n",
        "                      + \" Validation Loss: \" + str(np.mean(val_losses)))\n",
        "                \n",
        "                # append our results to a list for plotting later\n",
        "                step_plot.append(counter)\n",
        "                train_loss_plot.append(loss.item())\n",
        "                valid_loss_plot.append(np.mean(val_losses))\n",
        "                \n",
        "                if np.mean(val_losses) <= valid_loss_min: # Check if our validation loss reached a new minimum\n",
        "                    print(\"Val loss decreased. Save model.\")\n",
        "                    torch.save(net.state_dict(), 'model.pt')\n",
        "                    valid_loss_min = np.mean(val_losses)\n",
        "\n",
        "    # visualize our train loss and valid loss\n",
        "    plt.plot(step_plot, train_loss_plot, 'r--')\n",
        "    plt.plot(step_plot, valid_loss_plot, 'b-')\n",
        "    plt.legend(['Training Loss', 'Valid Loss'])\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebxfRYe7N3hs",
        "colab_type": "text"
      },
      "source": [
        "Now let's instantiate our model and define our parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M90xyeS5OBjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_hidden=512\n",
        "n_layers=3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz4XSUNMODzY",
        "colab_type": "text"
      },
      "source": [
        "Let's train our model! Let's also make sure to save the model when it reaches its lowest validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTgicO4ROIwN",
        "colab_type": "code",
        "outputId": "b16bb0d4-c339-42c9-dba8-8d9fa858c824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 80\n",
        "seq_length = 80\n",
        "n_epochs = 100\n",
        "\n",
        "# train the model\n",
        "train(net, \n",
        "      encoded,\n",
        "      epochs = n_epochs,\n",
        "      batch_size = batch_size,\n",
        "      seq_length = seq_length,\n",
        "      lr = 0.001,\n",
        "      print_every = 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Train Loss: 2.812223434448242 Validation Loss: 2.505034369391364\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 1 Train Loss: 2.3502137660980225 Validation Loss: 2.0453730273891138\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 1 Train Loss: 2.0833678245544434 Validation Loss: 1.8047665856979989\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 1 Train Loss: 1.9701809883117676 Validation Loss: 1.664698359128591\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 1 Train Loss: 1.8102957010269165 Validation Loss: 1.5769986929120243\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 1 Train Loss: 1.7510298490524292 Validation Loss: 1.5225916102125838\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 2 Train Loss: 1.656541347503662 Validation Loss: 1.4765633699056264\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 2 Train Loss: 1.6070683002471924 Validation Loss: 1.4422536717878807\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 2 Train Loss: 1.574215054512024 Validation Loss: 1.408131974774438\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 2 Train Loss: 1.5471094846725464 Validation Loss: 1.387004699255969\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 2 Train Loss: 1.510117530822754 Validation Loss: 1.3665114174018036\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 2 Train Loss: 1.4677362442016602 Validation Loss: 1.3518704627011273\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 2 Train Loss: 1.5016227960586548 Validation Loss: 1.3356275397378046\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 3 Train Loss: 1.4251903295516968 Validation Loss: 1.326533551151688\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 3 Train Loss: 1.456944465637207 Validation Loss: 1.3103851579331063\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 3 Train Loss: 1.3696566820144653 Validation Loss: 1.2979412127185512\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 3 Train Loss: 1.4251954555511475 Validation Loss: 1.2863386660008818\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 3 Train Loss: 1.3932937383651733 Validation Loss: 1.2767736041868054\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 3 Train Loss: 1.392328143119812 Validation Loss: 1.2674291423849158\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 3 Train Loss: 1.3659063577651978 Validation Loss: 1.26028566908192\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 4 Train Loss: 1.34934401512146 Validation Loss: 1.2517548461218138\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 4 Train Loss: 1.356225848197937 Validation Loss: 1.2453359011057261\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 4 Train Loss: 1.3530563116073608 Validation Loss: 1.2342077960839142\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 4 Train Loss: 1.3300482034683228 Validation Loss: 1.2290924143146824\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 4 Train Loss: 1.3174948692321777 Validation Loss: 1.2205772077715076\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 4 Train Loss: 1.3249773979187012 Validation Loss: 1.2168229544484936\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 5 Train Loss: 1.308565616607666 Validation Loss: 1.2121919747945424\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 5 Train Loss: 1.3207218647003174 Validation Loss: 1.20714710532008\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 5 Train Loss: 1.2942272424697876 Validation Loss: 1.2000648572638228\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 5 Train Loss: 1.2410086393356323 Validation Loss: 1.1955675321656305\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 5 Train Loss: 1.254756212234497 Validation Loss: 1.1902225581375327\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 5 Train Loss: 1.2700679302215576 Validation Loss: 1.1876762219377466\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 5 Train Loss: 1.2663812637329102 Validation Loss: 1.179228835814708\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 6 Train Loss: 1.1842491626739502 Validation Loss: 1.180908562363805\n",
            "Epoch: 6 Train Loss: 1.2162647247314453 Validation Loss: 1.1747287205747656\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 6 Train Loss: 1.2043120861053467 Validation Loss: 1.1692531447152834\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 6 Train Loss: 1.2559309005737305 Validation Loss: 1.1662210113293416\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 6 Train Loss: 1.2289735078811646 Validation Loss: 1.163870068820747\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 6 Train Loss: 1.2443116903305054 Validation Loss: 1.160054144021627\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 6 Train Loss: 1.1927778720855713 Validation Loss: 1.160067081451416\n",
            "Epoch: 7 Train Loss: 1.203066349029541 Validation Loss: 1.1548074773840002\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 7 Train Loss: 1.2559285163879395 Validation Loss: 1.1544058500109493\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 7 Train Loss: 1.1762096881866455 Validation Loss: 1.150220825865462\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 7 Train Loss: 1.243890643119812 Validation Loss: 1.1479094109019718\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 7 Train Loss: 1.2155534029006958 Validation Loss: 1.142677100929054\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 7 Train Loss: 1.242648959159851 Validation Loss: 1.1422225075799066\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 8 Train Loss: 1.1593800783157349 Validation Loss: 1.1423038002606984\n",
            "Epoch: 8 Train Loss: 1.1753513813018799 Validation Loss: 1.139440665373931\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 8 Train Loss: 1.1564435958862305 Validation Loss: 1.135428968313578\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 8 Train Loss: 1.1418746709823608 Validation Loss: 1.1331263136219334\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 8 Train Loss: 1.2052453756332397 Validation Loss: 1.130798177139179\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 8 Train Loss: 1.2147952318191528 Validation Loss: 1.1297437162012667\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 8 Train Loss: 1.191258192062378 Validation Loss: 1.1267658327076886\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 9 Train Loss: 1.1702438592910767 Validation Loss: 1.1290713532550916\n",
            "Epoch: 9 Train Loss: 1.1734931468963623 Validation Loss: 1.1248179351961292\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 9 Train Loss: 1.1420625448226929 Validation Loss: 1.1217375574885189\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 9 Train Loss: 1.2037299871444702 Validation Loss: 1.1197853088378906\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 9 Train Loss: 1.1947883367538452 Validation Loss: 1.1214946943360407\n",
            "Epoch: 9 Train Loss: 1.1680917739868164 Validation Loss: 1.1159478635401339\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 9 Train Loss: 1.1551222801208496 Validation Loss: 1.1187697052955627\n",
            "Epoch: 10 Train Loss: 1.1719181537628174 Validation Loss: 1.1147426850086934\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 10 Train Loss: 1.1814932823181152 Validation Loss: 1.1130831080514032\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 10 Train Loss: 1.1387028694152832 Validation Loss: 1.109770025755908\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 10 Train Loss: 1.1612036228179932 Validation Loss: 1.1112516006907902\n",
            "Epoch: 10 Train Loss: 1.1566115617752075 Validation Loss: 1.1059906337712262\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 10 Train Loss: 1.1882938146591187 Validation Loss: 1.1102120666890531\n",
            "Epoch: 11 Train Loss: 1.1419392824172974 Validation Loss: 1.1089536853738733\n",
            "Epoch: 11 Train Loss: 1.128708839416504 Validation Loss: 1.1065447668771486\n",
            "Epoch: 11 Train Loss: 1.114427089691162 Validation Loss: 1.1044718929239221\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 11 Train Loss: 1.133562445640564 Validation Loss: 1.104692331842474\n",
            "Epoch: 11 Train Loss: 1.1511046886444092 Validation Loss: 1.1022464874628428\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 11 Train Loss: 1.2130072116851807 Validation Loss: 1.1028303697302535\n",
            "Epoch: 11 Train Loss: 1.144841194152832 Validation Loss: 1.0991841635188542\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 12 Train Loss: 1.1135913133621216 Validation Loss: 1.099597982458166\n",
            "Epoch: 12 Train Loss: 1.108721137046814 Validation Loss: 1.096556571689812\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 12 Train Loss: 1.092911720275879 Validation Loss: 1.0962672668534357\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 12 Train Loss: 1.113759160041809 Validation Loss: 1.0960448948112693\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 12 Train Loss: 1.1060256958007812 Validation Loss: 1.0967773505159326\n",
            "Epoch: 12 Train Loss: 1.158448576927185 Validation Loss: 1.0933323566978042\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 12 Train Loss: 1.0872905254364014 Validation Loss: 1.0967262258400787\n",
            "Epoch: 13 Train Loss: 1.116875171661377 Validation Loss: 1.092406685287888\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 13 Train Loss: 1.1333246231079102 Validation Loss: 1.0914649367332458\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 13 Train Loss: 1.072658896446228 Validation Loss: 1.0895189497921918\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 13 Train Loss: 1.1298003196716309 Validation Loss: 1.091993254584235\n",
            "Epoch: 13 Train Loss: 1.1542134284973145 Validation Loss: 1.0879008850535832\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 13 Train Loss: 1.1369389295578003 Validation Loss: 1.0900744683033712\n",
            "Epoch: 14 Train Loss: 1.0866364240646362 Validation Loss: 1.0881947150101532\n",
            "Epoch: 14 Train Loss: 1.1312824487686157 Validation Loss: 1.0883822537757255\n",
            "Epoch: 14 Train Loss: 1.099545955657959 Validation Loss: 1.0866951201413129\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 14 Train Loss: 1.0869535207748413 Validation Loss: 1.0885526731207564\n",
            "Epoch: 14 Train Loss: 1.1097677946090698 Validation Loss: 1.0832840510316797\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 14 Train Loss: 1.160089373588562 Validation Loss: 1.0856054570223834\n",
            "Epoch: 14 Train Loss: 1.1371806859970093 Validation Loss: 1.0833677891138438\n",
            "Epoch: 15 Train Loss: 1.086240291595459 Validation Loss: 1.0842933058738708\n",
            "Epoch: 15 Train Loss: 1.0869096517562866 Validation Loss: 1.0812376222094975\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 15 Train Loss: 1.0767104625701904 Validation Loss: 1.0799888307983811\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 15 Train Loss: 1.1086910963058472 Validation Loss: 1.0798757027935337\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 15 Train Loss: 1.1076455116271973 Validation Loss: 1.0811127665880564\n",
            "Epoch: 15 Train Loss: 1.095620036125183 Validation Loss: 1.0794537744006596\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 15 Train Loss: 1.0436502695083618 Validation Loss: 1.0807387264999184\n",
            "Epoch: 16 Train Loss: 1.1087316274642944 Validation Loss: 1.079471636462856\n",
            "Epoch: 16 Train Loss: 1.1099884510040283 Validation Loss: 1.0766513879234727\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 16 Train Loss: 1.06210196018219 Validation Loss: 1.0781540419604327\n",
            "Epoch: 16 Train Loss: 1.0840264558792114 Validation Loss: 1.0789377012768306\n",
            "Epoch: 16 Train Loss: 1.0902557373046875 Validation Loss: 1.0759163885503202\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 16 Train Loss: 1.079636573791504 Validation Loss: 1.0765495187527425\n",
            "Epoch: 17 Train Loss: 1.0952882766723633 Validation Loss: 1.074147968678861\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 17 Train Loss: 1.0579999685287476 Validation Loss: 1.0767883674518481\n",
            "Epoch: 17 Train Loss: 1.103846549987793 Validation Loss: 1.0744310520790719\n",
            "Epoch: 17 Train Loss: 1.0911033153533936 Validation Loss: 1.0760088240778125\n",
            "Epoch: 17 Train Loss: 1.1463496685028076 Validation Loss: 1.0717727719126522\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 17 Train Loss: 1.1074554920196533 Validation Loss: 1.0741842453544204\n",
            "Epoch: 17 Train Loss: 1.0972334146499634 Validation Loss: 1.0715802956271816\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 18 Train Loss: 1.0635582208633423 Validation Loss: 1.0716697396458805\n",
            "Epoch: 18 Train Loss: 1.0287981033325195 Validation Loss: 1.0704655163996928\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 18 Train Loss: 1.0602713823318481 Validation Loss: 1.0708559042698629\n",
            "Epoch: 18 Train Loss: 1.096268892288208 Validation Loss: 1.0690328398266353\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 18 Train Loss: 1.0449756383895874 Validation Loss: 1.0699875242001302\n",
            "Epoch: 18 Train Loss: 1.0796756744384766 Validation Loss: 1.068440696677646\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 18 Train Loss: 1.0397933721542358 Validation Loss: 1.0696522996232316\n",
            "Epoch: 19 Train Loss: 1.0970708131790161 Validation Loss: 1.0694808041727222\n",
            "Epoch: 19 Train Loss: 1.083531379699707 Validation Loss: 1.0658108592033386\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 19 Train Loss: 1.033424973487854 Validation Loss: 1.0683953214336086\n",
            "Epoch: 19 Train Loss: 1.089188575744629 Validation Loss: 1.0692428753182694\n",
            "Epoch: 19 Train Loss: 1.074695348739624 Validation Loss: 1.0684451195033822\n",
            "Epoch: 19 Train Loss: 1.0869829654693604 Validation Loss: 1.0683381493027146\n",
            "Epoch: 20 Train Loss: 1.0693306922912598 Validation Loss: 1.0658302387675724\n",
            "Epoch: 20 Train Loss: 1.0104801654815674 Validation Loss: 1.0682970704259098\n",
            "Epoch: 20 Train Loss: 1.0692570209503174 Validation Loss: 1.065719271028364\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 20 Train Loss: 1.030250906944275 Validation Loss: 1.0678222807678017\n",
            "Epoch: 20 Train Loss: 1.1159874200820923 Validation Loss: 1.064218697515694\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 20 Train Loss: 1.0740995407104492 Validation Loss: 1.0668590197692047\n",
            "Epoch: 20 Train Loss: 1.0856640338897705 Validation Loss: 1.0650056439477045\n",
            "Epoch: 21 Train Loss: 1.0225545167922974 Validation Loss: 1.064768840332289\n",
            "Epoch: 21 Train Loss: 1.0363318920135498 Validation Loss: 1.0620224185892053\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 21 Train Loss: 1.035750389099121 Validation Loss: 1.062136868367324\n",
            "Epoch: 21 Train Loss: 1.079524040222168 Validation Loss: 1.065308769006987\n",
            "Epoch: 21 Train Loss: 1.0249308347702026 Validation Loss: 1.0630901520316665\n",
            "Epoch: 21 Train Loss: 1.0993809700012207 Validation Loss: 1.0638025757428762\n",
            "Epoch: 21 Train Loss: 1.0169851779937744 Validation Loss: 1.0610917677750458\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 22 Train Loss: 1.071755051612854 Validation Loss: 1.0620261197154586\n",
            "Epoch: 22 Train Loss: 1.0516737699508667 Validation Loss: 1.0588557873223279\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 22 Train Loss: 1.0213971138000488 Validation Loss: 1.061265845556517\n",
            "Epoch: 22 Train Loss: 1.0619311332702637 Validation Loss: 1.0625399198081042\n",
            "Epoch: 22 Train Loss: 1.0772134065628052 Validation Loss: 1.062720211776527\n",
            "Epoch: 22 Train Loss: 1.0606739521026611 Validation Loss: 1.0620972594699345\n",
            "Epoch: 23 Train Loss: 1.0530540943145752 Validation Loss: 1.0586228145135415\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 23 Train Loss: 1.0133455991744995 Validation Loss: 1.06088746318946\n",
            "Epoch: 23 Train Loss: 0.9969761371612549 Validation Loss: 1.0606830458383303\n",
            "Epoch: 23 Train Loss: 1.0433993339538574 Validation Loss: 1.0615245981796368\n",
            "Epoch: 23 Train Loss: 1.0865607261657715 Validation Loss: 1.0581537565669499\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 23 Train Loss: 1.0498781204223633 Validation Loss: 1.060610672106614\n",
            "Epoch: 23 Train Loss: 1.0078816413879395 Validation Loss: 1.0585414485351459\n",
            "Epoch: 24 Train Loss: 1.0314429998397827 Validation Loss: 1.0564867604423214\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 24 Train Loss: 1.0029492378234863 Validation Loss: 1.0563284448675208\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 24 Train Loss: 1.0074388980865479 Validation Loss: 1.0577028466237557\n",
            "Epoch: 24 Train Loss: 1.0537021160125732 Validation Loss: 1.0581686963906158\n",
            "Epoch: 24 Train Loss: 1.0767194032669067 Validation Loss: 1.0576374200550285\n",
            "Epoch: 24 Train Loss: 1.0541293621063232 Validation Loss: 1.0580739886374086\n",
            "Epoch: 24 Train Loss: 1.0115207433700562 Validation Loss: 1.0542964975576143\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 25 Train Loss: 1.0060691833496094 Validation Loss: 1.0562571673779875\n",
            "Epoch: 25 Train Loss: 1.0139528512954712 Validation Loss: 1.056052617124609\n",
            "Epoch: 25 Train Loss: 0.9870725274085999 Validation Loss: 1.054998802172171\n",
            "Epoch: 25 Train Loss: 1.0451613664627075 Validation Loss: 1.0565081488441777\n",
            "Epoch: 25 Train Loss: 1.0539543628692627 Validation Loss: 1.0589643088546958\n",
            "Epoch: 25 Train Loss: 1.0310946702957153 Validation Loss: 1.0542440076132078\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 26 Train Loss: 1.049583077430725 Validation Loss: 1.0541570871262937\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 26 Train Loss: 0.9997289180755615 Validation Loss: 1.0578144402117342\n",
            "Epoch: 26 Train Loss: 1.0598474740982056 Validation Loss: 1.0548270884397868\n",
            "Epoch: 26 Train Loss: 1.011120319366455 Validation Loss: 1.055048014666583\n",
            "Epoch: 26 Train Loss: 1.0688892602920532 Validation Loss: 1.0522779105482876\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 26 Train Loss: 1.0108190774917603 Validation Loss: 1.0576274451371785\n",
            "Epoch: 26 Train Loss: 1.019343376159668 Validation Loss: 1.0523542181865588\n",
            "Epoch: 27 Train Loss: 1.047501802444458 Validation Loss: 1.0527450845048234\n",
            "Epoch: 27 Train Loss: 1.0013797283172607 Validation Loss: 1.052962699451962\n",
            "Epoch: 27 Train Loss: 0.9925668835639954 Validation Loss: 1.0530669544194196\n",
            "Epoch: 27 Train Loss: 1.0557260513305664 Validation Loss: 1.05389085170385\n",
            "Epoch: 27 Train Loss: 1.027872085571289 Validation Loss: 1.0533772932516563\n",
            "Epoch: 27 Train Loss: 1.0598645210266113 Validation Loss: 1.0517810856973804\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 27 Train Loss: 1.051152229309082 Validation Loss: 1.0512679420612954\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 28 Train Loss: 0.995129406452179 Validation Loss: 1.053609321246276\n",
            "Epoch: 28 Train Loss: 1.0008831024169922 Validation Loss: 1.05101494853561\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 28 Train Loss: 1.0054340362548828 Validation Loss: 1.0517437337218105\n",
            "Epoch: 28 Train Loss: 1.0407699346542358 Validation Loss: 1.052119852723302\n",
            "Epoch: 28 Train Loss: 1.0273902416229248 Validation Loss: 1.054688489920384\n",
            "Epoch: 28 Train Loss: 1.0049961805343628 Validation Loss: 1.0500418318284523\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 29 Train Loss: 1.0460261106491089 Validation Loss: 1.0487283025238965\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 29 Train Loss: 0.9699917435646057 Validation Loss: 1.0513681511621218\n",
            "Epoch: 29 Train Loss: 1.02817964553833 Validation Loss: 1.0511726994772215\n",
            "Epoch: 29 Train Loss: 1.0136831998825073 Validation Loss: 1.0491071045398712\n",
            "Epoch: 29 Train Loss: 1.0573315620422363 Validation Loss: 1.0498651507738475\n",
            "Epoch: 29 Train Loss: 1.0123482942581177 Validation Loss: 1.0524442727501329\n",
            "Epoch: 29 Train Loss: 1.017487645149231 Validation Loss: 1.0514298653280414\n",
            "Epoch: 30 Train Loss: 1.057888388633728 Validation Loss: 1.0493776210256525\n",
            "Epoch: 30 Train Loss: 1.0244359970092773 Validation Loss: 1.049288599072276\n",
            "Epoch: 30 Train Loss: 1.0329585075378418 Validation Loss: 1.049012513579549\n",
            "Epoch: 30 Train Loss: 1.0017682313919067 Validation Loss: 1.0510980873494535\n",
            "Epoch: 30 Train Loss: 0.9892653822898865 Validation Loss: 1.0523360403808388\n",
            "Epoch: 30 Train Loss: 1.0375497341156006 Validation Loss: 1.0492457956881136\n",
            "Epoch: 30 Train Loss: 1.0076870918273926 Validation Loss: 1.0467678123229258\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 31 Train Loss: 1.0183401107788086 Validation Loss: 1.050019136151752\n",
            "Epoch: 31 Train Loss: 1.0371618270874023 Validation Loss: 1.048034953104483\n",
            "Epoch: 31 Train Loss: 1.0169365406036377 Validation Loss: 1.0475615488516319\n",
            "Epoch: 31 Train Loss: 0.9968116879463196 Validation Loss: 1.0474273989329468\n",
            "Epoch: 31 Train Loss: 1.031726598739624 Validation Loss: 1.0496436373607532\n",
            "Epoch: 31 Train Loss: 1.0413672924041748 Validation Loss: 1.047031894728944\n",
            "Epoch: 32 Train Loss: 1.0404889583587646 Validation Loss: 1.0488258584125623\n",
            "Epoch: 32 Train Loss: 0.9599254131317139 Validation Loss: 1.048963707041096\n",
            "Epoch: 32 Train Loss: 0.9668497443199158 Validation Loss: 1.0502821086226284\n",
            "Epoch: 32 Train Loss: 0.9649715423583984 Validation Loss: 1.045522465899184\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 32 Train Loss: 1.0158807039260864 Validation Loss: 1.0478706553175643\n",
            "Epoch: 32 Train Loss: 1.0197436809539795 Validation Loss: 1.0488677330919214\n",
            "Epoch: 32 Train Loss: 1.0258818864822388 Validation Loss: 1.0473504114795376\n",
            "Epoch: 33 Train Loss: 1.0337178707122803 Validation Loss: 1.048282639400379\n",
            "Epoch: 33 Train Loss: 0.9720494151115417 Validation Loss: 1.047871144236745\n",
            "Epoch: 33 Train Loss: 1.024110198020935 Validation Loss: 1.0468160485899127\n",
            "Epoch: 33 Train Loss: 0.9730893969535828 Validation Loss: 1.0503230143237758\n",
            "Epoch: 33 Train Loss: 1.0044965744018555 Validation Loss: 1.047004250255791\n",
            "Epoch: 33 Train Loss: 0.991736650466919 Validation Loss: 1.045137961974015\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 33 Train Loss: 0.9823847413063049 Validation Loss: 1.0431484998883427\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 34 Train Loss: 1.0121214389801025 Validation Loss: 1.048529811807581\n",
            "Epoch: 34 Train Loss: 1.053766131401062 Validation Loss: 1.0460256273682054\n",
            "Epoch: 34 Train Loss: 0.9957683086395264 Validation Loss: 1.0442790670974835\n",
            "Epoch: 34 Train Loss: 1.0138895511627197 Validation Loss: 1.0464735981580373\n",
            "Epoch: 34 Train Loss: 1.0300277471542358 Validation Loss: 1.0464368164539337\n",
            "Epoch: 34 Train Loss: 0.9711843729019165 Validation Loss: 1.0447658227907646\n",
            "Epoch: 35 Train Loss: 1.010533094406128 Validation Loss: 1.0463302514037571\n",
            "Epoch: 35 Train Loss: 0.9415304064750671 Validation Loss: 1.046978486550821\n",
            "Epoch: 35 Train Loss: 1.0198593139648438 Validation Loss: 1.0466868998231114\n",
            "Epoch: 35 Train Loss: 0.9695233702659607 Validation Loss: 1.0446314497573956\n",
            "Epoch: 35 Train Loss: 0.9937073588371277 Validation Loss: 1.0457178201224353\n",
            "Epoch: 35 Train Loss: 0.9759537577629089 Validation Loss: 1.0481156610153817\n",
            "Epoch: 35 Train Loss: 0.9798855781555176 Validation Loss: 1.0499048635766313\n",
            "Epoch: 36 Train Loss: 0.9904770851135254 Validation Loss: 1.046143958697448\n",
            "Epoch: 36 Train Loss: 0.9803576469421387 Validation Loss: 1.0471329761518013\n",
            "Epoch: 36 Train Loss: 1.0000145435333252 Validation Loss: 1.0448856547072127\n",
            "Epoch: 36 Train Loss: 0.991868793964386 Validation Loss: 1.0457479559086464\n",
            "Epoch: 36 Train Loss: 0.983487069606781 Validation Loss: 1.0489300610245884\n",
            "Epoch: 36 Train Loss: 1.0425572395324707 Validation Loss: 1.0448002936066807\n",
            "Epoch: 36 Train Loss: 0.9570152759552002 Validation Loss: 1.042128488018706\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 37 Train Loss: 1.0197880268096924 Validation Loss: 1.042567239419834\n",
            "Epoch: 37 Train Loss: 0.9983777403831482 Validation Loss: 1.0425401160845886\n",
            "Epoch: 37 Train Loss: 1.0129776000976562 Validation Loss: 1.0429975712621533\n",
            "Epoch: 37 Train Loss: 1.0035194158554077 Validation Loss: 1.0428704345548474\n",
            "Epoch: 37 Train Loss: 1.0092922449111938 Validation Loss: 1.0431698657370903\n",
            "Epoch: 37 Train Loss: 1.035658597946167 Validation Loss: 1.0428691554713894\n",
            "Epoch: 38 Train Loss: 1.022688627243042 Validation Loss: 1.0474415687290397\n",
            "Epoch: 38 Train Loss: 0.9402583837509155 Validation Loss: 1.0437918015428491\n",
            "Epoch: 38 Train Loss: 0.9931185245513916 Validation Loss: 1.0457832515239716\n",
            "Epoch: 38 Train Loss: 0.9633582234382629 Validation Loss: 1.0448225108352867\n",
            "Epoch: 38 Train Loss: 1.0130436420440674 Validation Loss: 1.044457196383863\n",
            "Epoch: 38 Train Loss: 0.9891344308853149 Validation Loss: 1.046343973359546\n",
            "Epoch: 38 Train Loss: 0.9950867295265198 Validation Loss: 1.0497710946443919\n",
            "Epoch: 39 Train Loss: 0.9937563538551331 Validation Loss: 1.0467142825191085\n",
            "Epoch: 39 Train Loss: 0.9607707858085632 Validation Loss: 1.0441671859573673\n",
            "Epoch: 39 Train Loss: 0.9890643358230591 Validation Loss: 1.0445626925777745\n",
            "Epoch: 39 Train Loss: 0.9802060723304749 Validation Loss: 1.044425825814943\n",
            "Epoch: 39 Train Loss: 0.9928193688392639 Validation Loss: 1.045537400084573\n",
            "Epoch: 39 Train Loss: 1.0606800317764282 Validation Loss: 1.0463762959918461\n",
            "Epoch: 39 Train Loss: 0.9353076219558716 Validation Loss: 1.0404537440957249\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 40 Train Loss: 0.9637514352798462 Validation Loss: 1.0415267606039305\n",
            "Epoch: 40 Train Loss: 1.0020171403884888 Validation Loss: 1.040186750727731\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 40 Train Loss: 1.0189157724380493 Validation Loss: 1.0425435393243223\n",
            "Epoch: 40 Train Loss: 0.9907262325286865 Validation Loss: 1.0404469564154342\n",
            "Epoch: 40 Train Loss: 0.9456214904785156 Validation Loss: 1.04090840913154\n",
            "Epoch: 40 Train Loss: 1.0317931175231934 Validation Loss: 1.0412182670992773\n",
            "Epoch: 41 Train Loss: 0.9610441327095032 Validation Loss: 1.0451204220990877\n",
            "Epoch: 41 Train Loss: 0.9203121066093445 Validation Loss: 1.0429155214412793\n",
            "Epoch: 41 Train Loss: 0.9955804944038391 Validation Loss: 1.044798708445317\n",
            "Epoch: 41 Train Loss: 0.9623773097991943 Validation Loss: 1.0415017524281063\n",
            "Epoch: 41 Train Loss: 0.9881103038787842 Validation Loss: 1.0418778077976123\n",
            "Epoch: 41 Train Loss: 0.97661954164505 Validation Loss: 1.046514481306076\n",
            "Epoch: 41 Train Loss: 0.9479658603668213 Validation Loss: 1.049323711846326\n",
            "Epoch: 42 Train Loss: 0.983330249786377 Validation Loss: 1.0458995748210598\n",
            "Epoch: 42 Train Loss: 0.9792322516441345 Validation Loss: 1.0450238029699068\n",
            "Epoch: 42 Train Loss: 0.9792836904525757 Validation Loss: 1.0455039659061947\n",
            "Epoch: 42 Train Loss: 0.9797441363334656 Validation Loss: 1.0432611937458451\n",
            "Epoch: 42 Train Loss: 0.9723404049873352 Validation Loss: 1.041548346345489\n",
            "Epoch: 42 Train Loss: 1.0500147342681885 Validation Loss: 1.045268775643529\n",
            "Epoch: 42 Train Loss: 0.956063449382782 Validation Loss: 1.0421314593907949\n",
            "Epoch: 43 Train Loss: 0.9691229462623596 Validation Loss: 1.0391181876530518\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 43 Train Loss: 1.0190317630767822 Validation Loss: 1.0412263008388314\n",
            "Epoch: 43 Train Loss: 0.9778905510902405 Validation Loss: 1.0412063791945174\n",
            "Epoch: 43 Train Loss: 0.9970370531082153 Validation Loss: 1.0381657763107404\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 43 Train Loss: 1.0029425621032715 Validation Loss: 1.0420950320926872\n",
            "Epoch: 43 Train Loss: 1.0064129829406738 Validation Loss: 1.0458365590185732\n",
            "Epoch: 44 Train Loss: 0.9131398797035217 Validation Loss: 1.0447763873113167\n",
            "Epoch: 44 Train Loss: 0.9197177886962891 Validation Loss: 1.0450218505150564\n",
            "Epoch: 44 Train Loss: 1.0135384798049927 Validation Loss: 1.0413872438508112\n",
            "Epoch: 44 Train Loss: 0.9827414751052856 Validation Loss: 1.0413370285485242\n",
            "Epoch: 44 Train Loss: 0.9996943473815918 Validation Loss: 1.042426487884006\n",
            "Epoch: 44 Train Loss: 0.994704008102417 Validation Loss: 1.0473349625999864\n",
            "Epoch: 44 Train Loss: 0.9554857015609741 Validation Loss: 1.0484765695559013\n",
            "Epoch: 45 Train Loss: 0.9929372668266296 Validation Loss: 1.047697948442923\n",
            "Epoch: 45 Train Loss: 0.9556551575660706 Validation Loss: 1.0456682548329637\n",
            "Epoch: 45 Train Loss: 0.9769496321678162 Validation Loss: 1.042927235364914\n",
            "Epoch: 45 Train Loss: 0.9614372849464417 Validation Loss: 1.0408528785447817\n",
            "Epoch: 45 Train Loss: 0.9538465142250061 Validation Loss: 1.04160452372319\n",
            "Epoch: 45 Train Loss: 1.0088948011398315 Validation Loss: 1.0456689354535695\n",
            "Epoch: 45 Train Loss: 1.000962495803833 Validation Loss: 1.0397423094994314\n",
            "Epoch: 46 Train Loss: 0.9465484619140625 Validation Loss: 1.0401329422319257\n",
            "Epoch: 46 Train Loss: 0.9888293743133545 Validation Loss: 1.0401337195087124\n",
            "Epoch: 46 Train Loss: 0.9828993082046509 Validation Loss: 1.0409686509016398\n",
            "Epoch: 46 Train Loss: 0.9135202765464783 Validation Loss: 1.0403598289231997\n",
            "Epoch: 46 Train Loss: 0.9815069437026978 Validation Loss: 1.0388629589531873\n",
            "Epoch: 46 Train Loss: 1.0221858024597168 Validation Loss: 1.0417943137723047\n",
            "Epoch: 47 Train Loss: 0.9720748662948608 Validation Loss: 1.044413703518945\n",
            "Epoch: 47 Train Loss: 0.9306272864341736 Validation Loss: 1.0426593358452256\n",
            "Epoch: 47 Train Loss: 0.978043794631958 Validation Loss: 1.0418297624265827\n",
            "Epoch: 47 Train Loss: 0.9635069370269775 Validation Loss: 1.0391854498837445\n",
            "Epoch: 47 Train Loss: 1.0112308263778687 Validation Loss: 1.0447195083708376\n",
            "Epoch: 47 Train Loss: 0.9608780741691589 Validation Loss: 1.0480887736823108\n",
            "Epoch: 47 Train Loss: 0.9011247158050537 Validation Loss: 1.0453268958104622\n",
            "Epoch: 48 Train Loss: 0.9462718367576599 Validation Loss: 1.0428474239400916\n",
            "Epoch: 48 Train Loss: 0.9592363834381104 Validation Loss: 1.0427485414453455\n",
            "Epoch: 48 Train Loss: 0.9643223285675049 Validation Loss: 1.0438949763774872\n",
            "Epoch: 48 Train Loss: 0.9348418712615967 Validation Loss: 1.03954407250559\n",
            "Epoch: 48 Train Loss: 0.96042799949646 Validation Loss: 1.0389603553591549\n",
            "Epoch: 48 Train Loss: 1.0189439058303833 Validation Loss: 1.043324066174997\n",
            "Epoch: 48 Train Loss: 0.9646309018135071 Validation Loss: 1.0400218053444013\n",
            "Epoch: 49 Train Loss: 0.926788866519928 Validation Loss: 1.037254395517143\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 49 Train Loss: 0.9834797382354736 Validation Loss: 1.0385955161339528\n",
            "Epoch: 49 Train Loss: 0.9485583305358887 Validation Loss: 1.0409210130975053\n",
            "Epoch: 49 Train Loss: 0.9398252367973328 Validation Loss: 1.0375391462364711\n",
            "Epoch: 49 Train Loss: 0.9723760485649109 Validation Loss: 1.037975469956527\n",
            "Epoch: 49 Train Loss: 0.998882532119751 Validation Loss: 1.0470327598017615\n",
            "Epoch: 50 Train Loss: 0.933218240737915 Validation Loss: 1.0419054997933876\n",
            "Epoch: 50 Train Loss: 0.9728854298591614 Validation Loss: 1.043302227516432\n",
            "Epoch: 50 Train Loss: 0.9653124809265137 Validation Loss: 1.040056330126685\n",
            "Epoch: 50 Train Loss: 0.9743137359619141 Validation Loss: 1.041691326611751\n",
            "Epoch: 50 Train Loss: 1.0015066862106323 Validation Loss: 1.0443383576096714\n",
            "Epoch: 50 Train Loss: 0.972344160079956 Validation Loss: 1.0451814153709926\n",
            "Epoch: 50 Train Loss: 0.907656192779541 Validation Loss: 1.0425407274349316\n",
            "Epoch: 51 Train Loss: 0.9223395586013794 Validation Loss: 1.0420415860575598\n",
            "Epoch: 51 Train Loss: 0.9144927263259888 Validation Loss: 1.0412359753170528\n",
            "Epoch: 51 Train Loss: 0.9128783345222473 Validation Loss: 1.0418436358103882\n",
            "Epoch: 51 Train Loss: 0.9808568358421326 Validation Loss: 1.039743497564986\n",
            "Epoch: 51 Train Loss: 0.9368834495544434 Validation Loss: 1.0416755974292755\n",
            "Epoch: 51 Train Loss: 1.0235979557037354 Validation Loss: 1.0445849976024113\n",
            "Epoch: 51 Train Loss: 0.9292346239089966 Validation Loss: 1.0377706716189514\n",
            "Epoch: 52 Train Loss: 0.9363090395927429 Validation Loss: 1.0383413417919263\n",
            "Epoch: 52 Train Loss: 0.9642401933670044 Validation Loss: 1.0406453851107005\n",
            "Epoch: 52 Train Loss: 0.9400781393051147 Validation Loss: 1.036540285961048\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 52 Train Loss: 0.9399034380912781 Validation Loss: 1.0370977931731455\n",
            "Epoch: 52 Train Loss: 0.9711905717849731 Validation Loss: 1.0353167089256081\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 52 Train Loss: 0.9792047739028931 Validation Loss: 1.0435181050687223\n",
            "Epoch: 53 Train Loss: 0.9482741355895996 Validation Loss: 1.042563026821291\n",
            "Epoch: 53 Train Loss: 0.8713085055351257 Validation Loss: 1.0453678095662915\n",
            "Epoch: 53 Train Loss: 0.9530239105224609 Validation Loss: 1.0390557509821814\n",
            "Epoch: 53 Train Loss: 0.983163595199585 Validation Loss: 1.0399511198739748\n",
            "Epoch: 53 Train Loss: 0.9083552360534668 Validation Loss: 1.043615875211922\n",
            "Epoch: 53 Train Loss: 0.9242500066757202 Validation Loss: 1.0476857311016805\n",
            "Epoch: 53 Train Loss: 0.9043756127357483 Validation Loss: 1.0425466282947644\n",
            "Epoch: 54 Train Loss: 0.925629198551178 Validation Loss: 1.0443595382007393\n",
            "Epoch: 54 Train Loss: 0.9411293864250183 Validation Loss: 1.0392063751414016\n",
            "Epoch: 54 Train Loss: 0.9191954731941223 Validation Loss: 1.0427781391788173\n",
            "Epoch: 54 Train Loss: 0.9782428741455078 Validation Loss: 1.0374016632904877\n",
            "Epoch: 54 Train Loss: 0.9489824771881104 Validation Loss: 1.0387212416610203\n",
            "Epoch: 54 Train Loss: 1.0167521238327026 Validation Loss: 1.042267167890394\n",
            "Epoch: 54 Train Loss: 0.9581454396247864 Validation Loss: 1.0372151506913674\n",
            "Epoch: 55 Train Loss: 0.978681206703186 Validation Loss: 1.0383275014323157\n",
            "Epoch: 55 Train Loss: 0.9581356644630432 Validation Loss: 1.0391659478883486\n",
            "Epoch: 55 Train Loss: 0.9724268317222595 Validation Loss: 1.0370110676095292\n",
            "Epoch: 55 Train Loss: 0.9664462208747864 Validation Loss: 1.0381556258008287\n",
            "Epoch: 55 Train Loss: 0.9684911370277405 Validation Loss: 1.0369314607736226\n",
            "Epoch: 55 Train Loss: 0.9166833758354187 Validation Loss: 1.0419260770887941\n",
            "Epoch: 56 Train Loss: 0.9700395464897156 Validation Loss: 1.0397131024180233\n",
            "Epoch: 56 Train Loss: 0.8871910572052002 Validation Loss: 1.042836835255494\n",
            "Epoch: 56 Train Loss: 0.9389064908027649 Validation Loss: 1.0388265389042932\n",
            "Epoch: 56 Train Loss: 0.9100096225738525 Validation Loss: 1.039814535830472\n",
            "Epoch: 56 Train Loss: 0.9272334575653076 Validation Loss: 1.0424739871476147\n",
            "Epoch: 56 Train Loss: 0.9532946944236755 Validation Loss: 1.0438159051779154\n",
            "Epoch: 56 Train Loss: 0.9096748232841492 Validation Loss: 1.039774226175772\n",
            "Epoch: 57 Train Loss: 0.9175063371658325 Validation Loss: 1.0394070261233561\n",
            "Epoch: 57 Train Loss: 0.8898438811302185 Validation Loss: 1.0428747969704706\n",
            "Epoch: 57 Train Loss: 0.9426048398017883 Validation Loss: 1.0415728341888737\n",
            "Epoch: 57 Train Loss: 0.9494854211807251 Validation Loss: 1.0400199882082037\n",
            "Epoch: 57 Train Loss: 0.9207784533500671 Validation Loss: 1.0402329572149225\n",
            "Epoch: 57 Train Loss: 0.9808804988861084 Validation Loss: 1.04320028424263\n",
            "Epoch: 57 Train Loss: 0.9101197123527527 Validation Loss: 1.041601039267875\n",
            "Epoch: 58 Train Loss: 0.9651206731796265 Validation Loss: 1.0372767883378107\n",
            "Epoch: 58 Train Loss: 0.9456204771995544 Validation Loss: 1.0377094737581305\n",
            "Epoch: 58 Train Loss: 0.9579715728759766 Validation Loss: 1.0370283553729187\n",
            "Epoch: 58 Train Loss: 0.9284425973892212 Validation Loss: 1.0399345352843001\n",
            "Epoch: 58 Train Loss: 0.9723464846611023 Validation Loss: 1.036785261856543\n",
            "Epoch: 58 Train Loss: 0.9619977474212646 Validation Loss: 1.0363781862967723\n",
            "Epoch: 59 Train Loss: 0.9597389698028564 Validation Loss: 1.0401774548195504\n",
            "Epoch: 59 Train Loss: 0.8670182824134827 Validation Loss: 1.041412405065588\n",
            "Epoch: 59 Train Loss: 1.0001543760299683 Validation Loss: 1.0371693332453031\n",
            "Epoch: 59 Train Loss: 0.9011744856834412 Validation Loss: 1.0380068807988554\n",
            "Epoch: 59 Train Loss: 0.9337394833564758 Validation Loss: 1.0435807785472355\n",
            "Epoch: 59 Train Loss: 0.927807629108429 Validation Loss: 1.0424962325676068\n",
            "Epoch: 59 Train Loss: 0.8943371772766113 Validation Loss: 1.0403758005515948\n",
            "Epoch: 60 Train Loss: 0.9013250470161438 Validation Loss: 1.039557872591792\n",
            "Epoch: 60 Train Loss: 0.9263408184051514 Validation Loss: 1.0406239894596305\n",
            "Epoch: 60 Train Loss: 0.8761928677558899 Validation Loss: 1.0430883531634871\n",
            "Epoch: 60 Train Loss: 0.9572780132293701 Validation Loss: 1.0402651139207788\n",
            "Epoch: 60 Train Loss: 0.9102957248687744 Validation Loss: 1.0398167766429283\n",
            "Epoch: 60 Train Loss: 0.9533632397651672 Validation Loss: 1.0393289440387004\n",
            "Epoch: 60 Train Loss: 0.8860408663749695 Validation Loss: 1.0394060756709125\n",
            "Epoch: 61 Train Loss: 0.9203779101371765 Validation Loss: 1.0352861969857603\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 61 Train Loss: 0.9303953051567078 Validation Loss: 1.0426942102006964\n",
            "Epoch: 61 Train Loss: 0.9429366588592529 Validation Loss: 1.0357368081002623\n",
            "Epoch: 61 Train Loss: 0.9515190124511719 Validation Loss: 1.0373188192779954\n",
            "Epoch: 61 Train Loss: 0.9496581554412842 Validation Loss: 1.0370251584697414\n",
            "Epoch: 61 Train Loss: 0.9342221617698669 Validation Loss: 1.0414592808968313\n",
            "Epoch: 62 Train Loss: 0.9584911465644836 Validation Loss: 1.0418207806509894\n",
            "Epoch: 62 Train Loss: 0.8773004412651062 Validation Loss: 1.041594829108264\n",
            "Epoch: 62 Train Loss: 0.9392082691192627 Validation Loss: 1.0392908993605021\n",
            "Epoch: 62 Train Loss: 0.8993107676506042 Validation Loss: 1.0385187179655642\n",
            "Epoch: 62 Train Loss: 0.9314921498298645 Validation Loss: 1.0469472545224268\n",
            "Epoch: 62 Train Loss: 0.9292585849761963 Validation Loss: 1.0407365834390796\n",
            "Epoch: 62 Train Loss: 0.9214237928390503 Validation Loss: 1.040487746934633\n",
            "Epoch: 63 Train Loss: 0.9121142029762268 Validation Loss: 1.0390230449470315\n",
            "Epoch: 63 Train Loss: 0.9061141014099121 Validation Loss: 1.0416151228788737\n",
            "Epoch: 63 Train Loss: 0.894916296005249 Validation Loss: 1.0418445564605094\n",
            "Epoch: 63 Train Loss: 0.9095394611358643 Validation Loss: 1.038038450318414\n",
            "Epoch: 63 Train Loss: 0.9587007164955139 Validation Loss: 1.0392583898595862\n",
            "Epoch: 63 Train Loss: 0.9822278022766113 Validation Loss: 1.0380828283928536\n",
            "Epoch: 63 Train Loss: 0.9254627227783203 Validation Loss: 1.038044572681994\n",
            "Epoch: 64 Train Loss: 0.9048452973365784 Validation Loss: 1.0362193133379962\n",
            "Epoch: 64 Train Loss: 0.9341773986816406 Validation Loss: 1.0402740457573452\n",
            "Epoch: 64 Train Loss: 0.9360530376434326 Validation Loss: 1.0357774665226807\n",
            "Epoch: 64 Train Loss: 0.9111056327819824 Validation Loss: 1.0373648163434621\n",
            "Epoch: 64 Train Loss: 0.9255692958831787 Validation Loss: 1.0395486137351475\n",
            "Epoch: 64 Train Loss: 0.9344558119773865 Validation Loss: 1.0422773417588826\n",
            "Epoch: 65 Train Loss: 0.8756901621818542 Validation Loss: 1.0447258619037834\n",
            "Epoch: 65 Train Loss: 0.8750824928283691 Validation Loss: 1.043436974287033\n",
            "Epoch: 65 Train Loss: 0.9703492522239685 Validation Loss: 1.0373745286786877\n",
            "Epoch: 65 Train Loss: 0.9111369252204895 Validation Loss: 1.0392473154776805\n",
            "Epoch: 65 Train Loss: 0.9171371459960938 Validation Loss: 1.0424078356575321\n",
            "Epoch: 65 Train Loss: 0.8959072232246399 Validation Loss: 1.0412456642937016\n",
            "Epoch: 65 Train Loss: 0.8384058475494385 Validation Loss: 1.0382131466994415\n",
            "Epoch: 66 Train Loss: 0.9046022295951843 Validation Loss: 1.041475254136163\n",
            "Epoch: 66 Train Loss: 0.9133159518241882 Validation Loss: 1.0380441844463348\n",
            "Epoch: 66 Train Loss: 0.9039485454559326 Validation Loss: 1.040304305585655\n",
            "Epoch: 66 Train Loss: 0.9310536980628967 Validation Loss: 1.0392871821248852\n",
            "Epoch: 66 Train Loss: 0.9642216563224792 Validation Loss: 1.0382656849719383\n",
            "Epoch: 66 Train Loss: 0.9774060249328613 Validation Loss: 1.039920617599745\n",
            "Epoch: 66 Train Loss: 0.9646821618080139 Validation Loss: 1.039058553205954\n",
            "Epoch: 67 Train Loss: 0.9241975545883179 Validation Loss: 1.0424224718196973\n",
            "Epoch: 67 Train Loss: 0.9306045770645142 Validation Loss: 1.0364852620137703\n",
            "Epoch: 67 Train Loss: 0.9283161163330078 Validation Loss: 1.0367889581499874\n",
            "Epoch: 67 Train Loss: 0.8777816891670227 Validation Loss: 1.0397731702069979\n",
            "Epoch: 67 Train Loss: 0.9417757391929626 Validation Loss: 1.037011513838897\n",
            "Epoch: 67 Train Loss: 0.9247388243675232 Validation Loss: 1.042765414392626\n",
            "Epoch: 68 Train Loss: 0.8737119436264038 Validation Loss: 1.0411543354794786\n",
            "Epoch: 68 Train Loss: 0.8844073414802551 Validation Loss: 1.0418651224793614\n",
            "Epoch: 68 Train Loss: 0.9338271617889404 Validation Loss: 1.0371372627245414\n",
            "Epoch: 68 Train Loss: 0.9257826805114746 Validation Loss: 1.038415187113994\n",
            "Epoch: 68 Train Loss: 0.8991116881370544 Validation Loss: 1.043902496228347\n",
            "Epoch: 68 Train Loss: 0.9359598755836487 Validation Loss: 1.0403493127307377\n",
            "Epoch: 68 Train Loss: 0.9135571122169495 Validation Loss: 1.0399315123622481\n",
            "Epoch: 69 Train Loss: 0.9299708604812622 Validation Loss: 1.0379615264969904\n",
            "Epoch: 69 Train Loss: 0.9416002035140991 Validation Loss: 1.0371763480676186\n",
            "Epoch: 69 Train Loss: 0.9085306525230408 Validation Loss: 1.0395070991000614\n",
            "Epoch: 69 Train Loss: 0.9237208366394043 Validation Loss: 1.0395610558020103\n",
            "Epoch: 69 Train Loss: 0.8965015411376953 Validation Loss: 1.0400492959731333\n",
            "Epoch: 69 Train Loss: 0.9677814245223999 Validation Loss: 1.0354668594695426\n",
            "Epoch: 69 Train Loss: 0.9410462975502014 Validation Loss: 1.0416786308224137\n",
            "Epoch: 70 Train Loss: 0.9212729930877686 Validation Loss: 1.0471358412020915\n",
            "Epoch: 70 Train Loss: 0.918232798576355 Validation Loss: 1.0380241637294356\n",
            "Epoch: 70 Train Loss: 0.9042571187019348 Validation Loss: 1.0394030213356018\n",
            "Epoch: 70 Train Loss: 0.9404541254043579 Validation Loss: 1.0400152472225395\n",
            "Epoch: 70 Train Loss: 0.9337992668151855 Validation Loss: 1.0385437341960702\n",
            "Epoch: 70 Train Loss: 0.9200680255889893 Validation Loss: 1.0386669506897797\n",
            "Epoch: 71 Train Loss: 0.9267029762268066 Validation Loss: 1.0378767147257522\n",
            "Epoch: 71 Train Loss: 0.9059233069419861 Validation Loss: 1.0474950783961527\n",
            "Epoch: 71 Train Loss: 0.961100697517395 Validation Loss: 1.0356780653064315\n",
            "Epoch: 71 Train Loss: 0.8759860396385193 Validation Loss: 1.0399402009474266\n",
            "Epoch: 71 Train Loss: 0.9083133935928345 Validation Loss: 1.0427118185404185\n",
            "Epoch: 71 Train Loss: 0.8826686143875122 Validation Loss: 1.036532051659919\n",
            "Epoch: 71 Train Loss: 0.9262329936027527 Validation Loss: 1.0411858985552918\n",
            "Epoch: 72 Train Loss: 0.9600536227226257 Validation Loss: 1.0421467648970115\n",
            "Epoch: 72 Train Loss: 0.9350522756576538 Validation Loss: 1.0385630718759589\n",
            "Epoch: 72 Train Loss: 0.9012972116470337 Validation Loss: 1.0416364621471714\n",
            "Epoch: 72 Train Loss: 0.9036126732826233 Validation Loss: 1.0398799221257906\n",
            "Epoch: 72 Train Loss: 0.9113621711730957 Validation Loss: 1.037070371009208\n",
            "Epoch: 72 Train Loss: 0.9667933583259583 Validation Loss: 1.0339289291484937\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 72 Train Loss: 0.8941532969474792 Validation Loss: 1.0362076396877702\n",
            "Epoch: 73 Train Loss: 0.904835045337677 Validation Loss: 1.0429232201060734\n",
            "Epoch: 73 Train Loss: 0.9196949005126953 Validation Loss: 1.038051403052098\n",
            "Epoch: 73 Train Loss: 0.927744448184967 Validation Loss: 1.0363139228240863\n",
            "Epoch: 73 Train Loss: 0.9646890759468079 Validation Loss: 1.0398948426182206\n",
            "Epoch: 73 Train Loss: 0.9843699932098389 Validation Loss: 1.0401001661210447\n",
            "Epoch: 73 Train Loss: 0.9399945139884949 Validation Loss: 1.039050094179205\n",
            "Epoch: 74 Train Loss: 0.9354814291000366 Validation Loss: 1.0384927843068097\n",
            "Epoch: 74 Train Loss: 0.9207165241241455 Validation Loss: 1.0393751247509107\n",
            "Epoch: 74 Train Loss: 0.9850254654884338 Validation Loss: 1.0375798086862307\n",
            "Epoch: 74 Train Loss: 0.9038314819335938 Validation Loss: 1.0388087920240454\n",
            "Epoch: 74 Train Loss: 0.8797929883003235 Validation Loss: 1.0436289479603638\n",
            "Epoch: 74 Train Loss: 0.8993541598320007 Validation Loss: 1.037540474453488\n",
            "Epoch: 74 Train Loss: 0.8937253355979919 Validation Loss: 1.0366242399086822\n",
            "Epoch: 75 Train Loss: 0.9494877457618713 Validation Loss: 1.0437051621643272\n",
            "Epoch: 75 Train Loss: 0.9602201581001282 Validation Loss: 1.0367210370463293\n",
            "Epoch: 75 Train Loss: 0.8495587706565857 Validation Loss: 1.0410564074645172\n",
            "Epoch: 75 Train Loss: 0.899635910987854 Validation Loss: 1.0408938132427834\n",
            "Epoch: 75 Train Loss: 0.9061587452888489 Validation Loss: 1.042959558802682\n",
            "Epoch: 75 Train Loss: 0.922372579574585 Validation Loss: 1.0337444825752362\n",
            "Val loss decreased. Save model.\n",
            "Epoch: 75 Train Loss: 0.898688554763794 Validation Loss: 1.0410125070327036\n",
            "Epoch: 76 Train Loss: 0.8776974678039551 Validation Loss: 1.038987868540996\n",
            "Epoch: 76 Train Loss: 0.9383248090744019 Validation Loss: 1.0371958726161234\n",
            "Epoch: 76 Train Loss: 0.9360907077789307 Validation Loss: 1.035681897723997\n",
            "Epoch: 76 Train Loss: 0.914053201675415 Validation Loss: 1.038416414647489\n",
            "Epoch: 76 Train Loss: 0.9443781971931458 Validation Loss: 1.0429503458577234\n",
            "Epoch: 76 Train Loss: 0.9699909687042236 Validation Loss: 1.0418411404699892\n",
            "Epoch: 77 Train Loss: 0.8991374969482422 Validation Loss: 1.0381971637944918\n",
            "Epoch: 77 Train Loss: 0.9222161173820496 Validation Loss: 1.0401985153958604\n",
            "Epoch: 77 Train Loss: 0.971286952495575 Validation Loss: 1.0375886301736574\n",
            "Epoch: 77 Train Loss: 0.9335013628005981 Validation Loss: 1.0396653059366587\n",
            "Epoch: 77 Train Loss: 0.8984713554382324 Validation Loss: 1.042566972809869\n",
            "Epoch: 77 Train Loss: 0.9427376389503479 Validation Loss: 1.0369684253190015\n",
            "Epoch: 77 Train Loss: 0.8847925662994385 Validation Loss: 1.0373578136031691\n",
            "Epoch: 78 Train Loss: 0.8811500668525696 Validation Loss: 1.0407114592758384\n",
            "Epoch: 78 Train Loss: 0.8987317085266113 Validation Loss: 1.0394654668666221\n",
            "Epoch: 78 Train Loss: 0.9154510498046875 Validation Loss: 1.0427653419004905\n",
            "Epoch: 78 Train Loss: 0.9153986573219299 Validation Loss: 1.044231140130275\n",
            "Epoch: 78 Train Loss: 0.8662843108177185 Validation Loss: 1.0403945510451857\n",
            "Epoch: 78 Train Loss: 0.9563182592391968 Validation Loss: 1.0374913207582526\n",
            "Epoch: 78 Train Loss: 0.8521207571029663 Validation Loss: 1.039697382901166\n",
            "Epoch: 79 Train Loss: 0.8739370107650757 Validation Loss: 1.043634994609936\n",
            "Epoch: 79 Train Loss: 0.9277754426002502 Validation Loss: 1.0373153291844033\n",
            "Epoch: 79 Train Loss: 0.9444234371185303 Validation Loss: 1.0377309225700997\n",
            "Epoch: 79 Train Loss: 0.8937877416610718 Validation Loss: 1.0389607838682227\n",
            "Epoch: 79 Train Loss: 0.8803331255912781 Validation Loss: 1.0402960310111176\n",
            "Epoch: 79 Train Loss: 0.9453933238983154 Validation Loss: 1.0339342122142379\n",
            "Epoch: 80 Train Loss: 0.9142412543296814 Validation Loss: 1.0416974248112858\n",
            "Epoch: 80 Train Loss: 0.8856572508811951 Validation Loss: 1.0377709462835982\n",
            "Epoch: 80 Train Loss: 0.9326439499855042 Validation Loss: 1.034849165259181\n",
            "Epoch: 80 Train Loss: 0.9028989672660828 Validation Loss: 1.040761742237452\n",
            "Epoch: 80 Train Loss: 0.9414341449737549 Validation Loss: 1.0410982837548126\n",
            "Epoch: 80 Train Loss: 0.941714882850647 Validation Loss: 1.0360197110756024\n",
            "Epoch: 80 Train Loss: 0.8551185727119446 Validation Loss: 1.0350575648449563\n",
            "Epoch: 81 Train Loss: 0.9142763614654541 Validation Loss: 1.0387546025417946\n",
            "Epoch: 81 Train Loss: 0.9220192432403564 Validation Loss: 1.0395916289574392\n",
            "Epoch: 81 Train Loss: 0.8558712005615234 Validation Loss: 1.0425411011721637\n",
            "Epoch: 81 Train Loss: 0.9331514239311218 Validation Loss: 1.0444179161174878\n",
            "Epoch: 81 Train Loss: 0.9094653129577637 Validation Loss: 1.0431301803202242\n",
            "Epoch: 81 Train Loss: 0.9609925746917725 Validation Loss: 1.0367773457153424\n",
            "Epoch: 81 Train Loss: 0.8867545127868652 Validation Loss: 1.0391184647341032\n",
            "Epoch: 82 Train Loss: 0.8605813384056091 Validation Loss: 1.042547223535744\n",
            "Epoch: 82 Train Loss: 0.9111971855163574 Validation Loss: 1.0385206825024373\n",
            "Epoch: 82 Train Loss: 0.9278630614280701 Validation Loss: 1.0385771756236617\n",
            "Epoch: 82 Train Loss: 0.9097268581390381 Validation Loss: 1.0385989297080684\n",
            "Epoch: 82 Train Loss: 0.8446523547172546 Validation Loss: 1.0384967520430282\n",
            "Epoch: 82 Train Loss: 0.998994767665863 Validation Loss: 1.036836321289475\n",
            "Epoch: 83 Train Loss: 0.8795613050460815 Validation Loss: 1.0388945880773905\n",
            "Epoch: 83 Train Loss: 0.8889056444168091 Validation Loss: 1.0399884790987581\n",
            "Epoch: 83 Train Loss: 0.91008460521698 Validation Loss: 1.0379687781269487\n",
            "Epoch: 83 Train Loss: 0.8897742629051208 Validation Loss: 1.0408610830435883\n",
            "Epoch: 83 Train Loss: 0.8838842511177063 Validation Loss: 1.038919163716806\n",
            "Epoch: 83 Train Loss: 0.8928481340408325 Validation Loss: 1.037858774533143\n",
            "Epoch: 83 Train Loss: 0.852672278881073 Validation Loss: 1.0375562100797087\n",
            "Epoch: 84 Train Loss: 0.9412232041358948 Validation Loss: 1.0364218096475344\n",
            "Epoch: 84 Train Loss: 0.9123799800872803 Validation Loss: 1.037296371685492\n",
            "Epoch: 84 Train Loss: 0.8529146313667297 Validation Loss: 1.0427707739778467\n",
            "Epoch: 84 Train Loss: 0.8805290460586548 Validation Loss: 1.0423122128924809\n",
            "Epoch: 84 Train Loss: 0.9264599084854126 Validation Loss: 1.0460797153614663\n",
            "Epoch: 84 Train Loss: 0.9693487286567688 Validation Loss: 1.03675536046157\n",
            "Epoch: 84 Train Loss: 0.8963866233825684 Validation Loss: 1.0354507686318577\n",
            "Epoch: 85 Train Loss: 0.8641911149024963 Validation Loss: 1.042085662081435\n",
            "Epoch: 85 Train Loss: 0.9477501511573792 Validation Loss: 1.0366714556474943\n",
            "Epoch: 85 Train Loss: 0.9258365631103516 Validation Loss: 1.0437037558168978\n",
            "Epoch: 85 Train Loss: 0.8676920533180237 Validation Loss: 1.0389783245486182\n",
            "Epoch: 85 Train Loss: 0.8627166748046875 Validation Loss: 1.042872406340934\n",
            "Epoch: 85 Train Loss: 0.9359301924705505 Validation Loss: 1.0402930668882422\n",
            "Epoch: 86 Train Loss: 0.8586401343345642 Validation Loss: 1.0377064548634194\n",
            "Epoch: 86 Train Loss: 0.8972207903862 Validation Loss: 1.04241945131405\n",
            "Epoch: 86 Train Loss: 0.8943645358085632 Validation Loss: 1.034572211471764\n",
            "Epoch: 86 Train Loss: 0.8971533179283142 Validation Loss: 1.0382447943494126\n",
            "Epoch: 86 Train Loss: 0.883979320526123 Validation Loss: 1.0378098528127413\n",
            "Epoch: 86 Train Loss: 0.9075419902801514 Validation Loss: 1.037969162335267\n",
            "Epoch: 86 Train Loss: 0.8791555762290955 Validation Loss: 1.0416532062195443\n",
            "Epoch: 87 Train Loss: 0.9196739792823792 Validation Loss: 1.0369418215107273\n",
            "Epoch: 87 Train Loss: 0.9438092112541199 Validation Loss: 1.038716733455658\n",
            "Epoch: 87 Train Loss: 0.8784511685371399 Validation Loss: 1.0466795837556995\n",
            "Epoch: 87 Train Loss: 0.8809582591056824 Validation Loss: 1.0439745477727942\n",
            "Epoch: 87 Train Loss: 0.8805327415466309 Validation Loss: 1.0430915420119826\n",
            "Epoch: 87 Train Loss: 0.948302149772644 Validation Loss: 1.0375005022899524\n",
            "Epoch: 87 Train Loss: 0.9089390635490417 Validation Loss: 1.0388089700325116\n",
            "Epoch: 88 Train Loss: 0.8307046294212341 Validation Loss: 1.0419878017258\n",
            "Epoch: 88 Train Loss: 0.9526677131652832 Validation Loss: 1.0358846558106911\n",
            "Epoch: 88 Train Loss: 0.94158536195755 Validation Loss: 1.0370648954365704\n",
            "Epoch: 88 Train Loss: 0.8791093230247498 Validation Loss: 1.0358535977634225\n",
            "Epoch: 88 Train Loss: 0.8606021404266357 Validation Loss: 1.0386239612424695\n",
            "Epoch: 88 Train Loss: 0.9213997721672058 Validation Loss: 1.0386603160484418\n",
            "Epoch: 89 Train Loss: 0.8426077365875244 Validation Loss: 1.0434140675776713\n",
            "Epoch: 89 Train Loss: 0.8385538458824158 Validation Loss: 1.0419062980123468\n",
            "Epoch: 89 Train Loss: 0.9358947277069092 Validation Loss: 1.0374622030838117\n",
            "Epoch: 89 Train Loss: 0.8901399374008179 Validation Loss: 1.0380356279579368\n",
            "Epoch: 89 Train Loss: 0.9032238721847534 Validation Loss: 1.0407614232720555\n",
            "Epoch: 89 Train Loss: 0.9031208753585815 Validation Loss: 1.0430118216050637\n",
            "Epoch: 89 Train Loss: 0.8623605966567993 Validation Loss: 1.0406360932298608\n",
            "Epoch: 90 Train Loss: 0.9447464942932129 Validation Loss: 1.0372511277327667\n",
            "Epoch: 90 Train Loss: 0.9050843715667725 Validation Loss: 1.0445479428445972\n",
            "Epoch: 90 Train Loss: 0.8743579387664795 Validation Loss: 1.0455328435511202\n",
            "Epoch: 90 Train Loss: 0.846445620059967 Validation Loss: 1.041635830660124\n",
            "Epoch: 90 Train Loss: 0.8710426092147827 Validation Loss: 1.0406794862167255\n",
            "Epoch: 90 Train Loss: 0.9459620118141174 Validation Loss: 1.0340799020754325\n",
            "Epoch: 90 Train Loss: 0.9126350283622742 Validation Loss: 1.0376026686784383\n",
            "Epoch: 91 Train Loss: 0.8820329308509827 Validation Loss: 1.0399813845350936\n",
            "Epoch: 91 Train Loss: 0.9612172842025757 Validation Loss: 1.0401396968880214\n",
            "Epoch: 91 Train Loss: 0.8883860111236572 Validation Loss: 1.0402112627351605\n",
            "Epoch: 91 Train Loss: 0.8986889123916626 Validation Loss: 1.037825948483235\n",
            "Epoch: 91 Train Loss: 0.8950385451316833 Validation Loss: 1.0445009751899823\n",
            "Epoch: 91 Train Loss: 0.9683789610862732 Validation Loss: 1.0348808805684786\n",
            "Epoch: 92 Train Loss: 0.8199688792228699 Validation Loss: 1.0387944563015088\n",
            "Epoch: 92 Train Loss: 0.8777948021888733 Validation Loss: 1.0382756373366795\n",
            "Epoch: 92 Train Loss: 0.9329749345779419 Validation Loss: 1.036966851434192\n",
            "Epoch: 92 Train Loss: 0.8705233931541443 Validation Loss: 1.0408810393230334\n",
            "Epoch: 92 Train Loss: 0.924119770526886 Validation Loss: 1.0398087863986556\n",
            "Epoch: 92 Train Loss: 0.8975061774253845 Validation Loss: 1.0395218117817029\n",
            "Epoch: 92 Train Loss: 0.8932768106460571 Validation Loss: 1.0407933028968606\n",
            "Epoch: 93 Train Loss: 0.9355062246322632 Validation Loss: 1.0365517268309723\n",
            "Epoch: 93 Train Loss: 0.8908310532569885 Validation Loss: 1.0403935603193335\n",
            "Epoch: 93 Train Loss: 0.8687003254890442 Validation Loss: 1.0476786719786155\n",
            "Epoch: 93 Train Loss: 0.814068615436554 Validation Loss: 1.0416215416547414\n",
            "Epoch: 93 Train Loss: 0.8702502250671387 Validation Loss: 1.037327859047297\n",
            "Epoch: 93 Train Loss: 0.9540190100669861 Validation Loss: 1.0387152860293518\n",
            "Epoch: 93 Train Loss: 0.8658843040466309 Validation Loss: 1.0338057287641473\n",
            "Epoch: 94 Train Loss: 0.8371503949165344 Validation Loss: 1.0414054877049215\n",
            "Epoch: 94 Train Loss: 0.9322970509529114 Validation Loss: 1.0369554162025452\n",
            "Epoch: 94 Train Loss: 0.878013551235199 Validation Loss: 1.0390733222703676\n",
            "Epoch: 94 Train Loss: 0.8980822563171387 Validation Loss: 1.0386806146518603\n",
            "Epoch: 94 Train Loss: 0.8832871317863464 Validation Loss: 1.0451212146797695\n",
            "Epoch: 94 Train Loss: 0.9837783575057983 Validation Loss: 1.0392458793279287\n",
            "Epoch: 95 Train Loss: 0.8579566478729248 Validation Loss: 1.0352720706849485\n",
            "Epoch: 95 Train Loss: 0.8831660747528076 Validation Loss: 1.0378347860800254\n",
            "Epoch: 95 Train Loss: 0.884162187576294 Validation Loss: 1.0377489594188896\n",
            "Epoch: 95 Train Loss: 0.8889485001564026 Validation Loss: 1.0408581326136719\n",
            "Epoch: 95 Train Loss: 0.9110662341117859 Validation Loss: 1.0416415655935132\n",
            "Epoch: 95 Train Loss: 0.9050557613372803 Validation Loss: 1.0432130420530163\n",
            "Epoch: 95 Train Loss: 0.9425153136253357 Validation Loss: 1.0421527036138483\n",
            "Epoch: 96 Train Loss: 0.9057247042655945 Validation Loss: 1.039253887292501\n",
            "Epoch: 96 Train Loss: 0.8748350739479065 Validation Loss: 1.0409715578362748\n",
            "Epoch: 96 Train Loss: 0.8594370484352112 Validation Loss: 1.0452318078762777\n",
            "Epoch: 96 Train Loss: 0.8261159658432007 Validation Loss: 1.04248492943274\n",
            "Epoch: 96 Train Loss: 0.8917874097824097 Validation Loss: 1.0388573158431698\n",
            "Epoch: 96 Train Loss: 0.9141325354576111 Validation Loss: 1.0390585443458042\n",
            "Epoch: 96 Train Loss: 0.8604204654693604 Validation Loss: 1.0339087408942145\n",
            "Epoch: 97 Train Loss: 0.88968825340271 Validation Loss: 1.036510692254917\n",
            "Epoch: 97 Train Loss: 0.8988492488861084 Validation Loss: 1.0368374014222943\n",
            "Epoch: 97 Train Loss: 0.9236007928848267 Validation Loss: 1.0403388158695117\n",
            "Epoch: 97 Train Loss: 0.8828359842300415 Validation Loss: 1.037516218584937\n",
            "Epoch: 97 Train Loss: 0.8617744445800781 Validation Loss: 1.0391602709486678\n",
            "Epoch: 97 Train Loss: 0.9880883097648621 Validation Loss: 1.0386735555287954\n",
            "Epoch: 98 Train Loss: 1.023026943206787 Validation Loss: 1.0387147713351894\n",
            "Epoch: 98 Train Loss: 0.8719534277915955 Validation Loss: 1.0434555975166526\n",
            "Epoch: 98 Train Loss: 0.8862130045890808 Validation Loss: 1.0399756318814046\n",
            "Epoch: 98 Train Loss: 0.9299364686012268 Validation Loss: 1.0393063127994537\n",
            "Epoch: 98 Train Loss: 0.9008432030677795 Validation Loss: 1.0417841656787976\n",
            "Epoch: 98 Train Loss: 0.8487319350242615 Validation Loss: 1.0472651148164593\n",
            "Epoch: 98 Train Loss: 0.8969284892082214 Validation Loss: 1.0434789786467682\n",
            "Epoch: 99 Train Loss: 0.8814056515693665 Validation Loss: 1.0348753099505965\n",
            "Epoch: 99 Train Loss: 0.9058794379234314 Validation Loss: 1.0447722463994413\n",
            "Epoch: 99 Train Loss: 0.8516479730606079 Validation Loss: 1.0459474147977055\n",
            "Epoch: 99 Train Loss: 0.8272699117660522 Validation Loss: 1.038941488878147\n",
            "Epoch: 99 Train Loss: 0.862989604473114 Validation Loss: 1.0434405207633972\n",
            "Epoch: 99 Train Loss: 0.9105970859527588 Validation Loss: 1.0415695357967067\n",
            "Epoch: 99 Train Loss: 0.884742021560669 Validation Loss: 1.0378171326340855\n",
            "Epoch: 100 Train Loss: 0.8833299279212952 Validation Loss: 1.0401963656013076\n",
            "Epoch: 100 Train Loss: 0.9109153747558594 Validation Loss: 1.0398549687218022\n",
            "Epoch: 100 Train Loss: 0.8586261868476868 Validation Loss: 1.0413771932189528\n",
            "Epoch: 100 Train Loss: 0.9133558869361877 Validation Loss: 1.0351450829892546\n",
            "Epoch: 100 Train Loss: 0.8307735919952393 Validation Loss: 1.0386962681203276\n",
            "Epoch: 100 Train Loss: 0.978076696395874 Validation Loss: 1.0383604209165316\n",
            "Epoch: 100 Train Loss: 0.9911354184150696 Validation Loss: 1.0394239449823224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEGCAYAAABYV4NmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHMdxnFD4fyrZko6tuZZZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PasDX_DNYvrL",
        "colab_type": "text"
      },
      "source": [
        "As seen in the plot above, the validation loss stops decreasing at some point during the training. \n",
        "After the network has been trained let's add functions for top-k sampling and for generating text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y815achkZCgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "\n",
        "        x = torch.from_numpy(x)\n",
        "        # Use functional library's one_hot function to one-hot encode our data.\n",
        "        x = F.one_hot(x, num_classes=len(net.chars))\n",
        "        inputs = x.float()\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = net(inputs, h)\n",
        "        p = F.softmax(out, dim=1).data\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu()\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "      \n",
        "        return net.int2char[char], h\n",
        "\n",
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXvYnXyxZGUW",
        "colab_type": "text"
      },
      "source": [
        "Let's create our song that starts with the word love! But first let's get the mean song length so we'll know how long our song needs to be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUwp6SXUZLVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_song_length = dataset['text'].apply(lambda x:len(x)).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfKFcs8WC-GP",
        "colab_type": "text"
      },
      "source": [
        "With this length, let's cast it to an integer and let's finally create our love song!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhS1cVM4C-gh",
        "colab_type": "code",
        "outputId": "b26555b5-cf42-4bf4-afc5-74557424640c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "net.load_state_dict(torch.load('model.pt'))\n",
        "print(sample(net, int(mean_song_length), prime='Hanika', top_k=2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hanikation  \n",
            "And the stars are shaking down on you again  \n",
            "I'm a love like a shame  \n",
            "I wanna be your lovin' way  \n",
            "  \n",
            "I want the world and I want you  \n",
            "And if I'm always with you  \n",
            "I want to be loved  \n",
            "I'm in love with you  \n",
            "I wanna feel what love is  \n",
            "I know I know you can show me  \n",
            "I wanna feel what love is  \n",
            "I'll do anything you want to make me cry  \n",
            "I don't want to be with you more than your lovin' way  \n",
            "I want you to share my love with you  \n",
            "  \n",
            "I want to show you what love is if you want me  \n",
            "I don't want you to stop my life  \n",
            "  \n",
            "You can take me away  \n",
            "I'm not trying not to leave your faith in me  \n",
            "I'm goin' a little longer  \n",
            "I can't believe that I could never find  \n",
            "I didn't know I'm always wrong  \n",
            "  \n",
            "And I'm so in love with you  \n",
            "I want your love tonight  \n",
            "I will always stop the world  \n",
            "To be with you and I can't live without  \n",
            "I wanna feel you  \n",
            "I'll buy you a little baby  \n",
            "I'll be your life and tell you that I don't wanna be  \n",
            "I don't want you to stop and tell you that I'm a little bit of mind  \n",
            "  \n",
            "[Chorus]  \n",
            "I love to love ya  \n",
            "I love you  \n",
            "  \n",
            "I'm a lonely one  \n",
            "And I want to share it with you  \n",
            "I don't want to \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae-4xs5oE96s",
        "colab_type": "code",
        "outputId": "f909175a-89f6-4cdb-a303-994097210616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        }
      },
      "source": [
        "print(sample(net, int(mean_song_length), prime='Hanika', top_k=2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hanika  \n",
            "Always livin' in a dangerous time\n",
            "\n",
            " We got to get out of this love of my life  \n",
            "I'm a lover start the word that I was standing in the morning  \n",
            "I want to be the one to say that you love me  \n",
            "  \n",
            "I wanna be your man  \n",
            "I know you love  \n",
            "I want to stay this way in love with you\n",
            "\n",
            " I want to say it again  \n",
            "  \n",
            "[Chorus:]  \n",
            "  \n",
            "I love to love you baby  \n",
            "I love to love you baby  \n",
            "I love to love you baby  \n",
            "I love to love you baby (I love all love me)  \n",
            "I'll be there for you  \n",
            "  \n",
            "I was a love like the woman in love  \n",
            "With the stars above  \n",
            "And the stars are gone  \n",
            "And I'm standing on the road  \n",
            "I will love you  \n",
            "I will always love you\n",
            "\n",
            " I've got to be the one to be with you  \n",
            "I'm gonna give you all my love all my love  \n",
            "I wanna little love  \n",
            "Give you love give you love  \n",
            "  \n",
            "I've got my love to keep me warm  \n",
            "  \n",
            "I wanna little love  \n",
            "Give me love  \n",
            "  \n",
            "I wanna be loved  \n",
            "I wanna be loved  \n",
            "I'm in love love love  \n",
            "I wanna little love  \n",
            "Give me love  \n",
            "Give me love  \n",
            "Give me love  \n",
            "Give you love  \n",
            "Give me love  \n",
            "Give me love  \n",
            "  \n",
            "I'm gonna get you  \n",
            "  \n",
            "I love to love ya  \n",
            "Gonna love you  \n",
            "Girl let me love you  \n",
            "Girl love me now  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxOPNrf4GSc_",
        "colab_type": "code",
        "outputId": "b8050d75-3c8b-44c0-ff5a-2a37a24852f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(sample(net, int(mean_song_length), prime='Hanika', top_k=2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hanika  \n",
            "I wanna be your man  \n",
            "  \n",
            "I'm gonna give you all my loving  \n",
            "I'm gonna give you all my loving  \n",
            "I got love took you  \n",
            "I wanna be inside your superlove  \n",
            "I love to love you baby  \n",
            "  \n",
            "I love this love oh yeah  \n",
            "I love to love you baby  \n",
            "  \n",
            "I'll be there for you  \n",
            "  \n",
            "I want to be your lover baby  \n",
            "I wanna be your lover now  \n",
            "  \n",
            "You're the one that makes me feel like I do  \n",
            "I want to be your lover boy  \n",
            "I wanna feel what love is I know you love  \n",
            "I want your love I want your love  \n",
            "I'll do anything you want  \n",
            "I want you to show me  \n",
            "  \n",
            "I wanna be loved  \n",
            "I just called to say I love you  \n",
            "I wanna feel what love is  \n",
            "I wanna be loved  \n",
            "I wanna be loved by you  \n",
            "  \n",
            "I wanna be loved  \n",
            "I want you to know I do  \n",
            "  \n",
            "[Chorus]  \n",
            "  \n",
            "I'm gonna make you love me  \n",
            "  \n",
            "I want the whole world to know  \n",
            "What it is that I've got  \n",
            "I'm gonna love you more than I do  \n",
            "I wanna be inside your superlove  \n",
            "I need to be with you  \n",
            "  \n",
            "I wanna love you baby  \n",
            "I don't wanna love anymore  \n",
            "  \n",
            "I don't wanna be with you  \n",
            "  \n",
            "I'm gonna make you love me  \n",
            "  \n",
            "I wanna little love  \n",
            "Get down make love  \n",
            "Give me love  \n",
            "Save your love  \n",
            "Save your love  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}